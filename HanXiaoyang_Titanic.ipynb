{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 经典又兼具备趣味性的Kaggle案例[泰坦尼克号问题](https://www.kaggle.com/c/titanic)\n",
    "大家都熟悉的『Jack and Rose』的故事，豪华游艇倒了，大家都惊恐逃生，可是救生艇的数量有限，无法人人都有，副船长发话了『lady and kid first！』，所以是否获救其实并非随机，而是基于一些背景有rank先后的。<br>\n",
    "训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。<br>\n",
    "对，这是一个二分类问题，很多分类算法都可以解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>看看数据长什么样</font>**<br>\n",
    "还是用pandas加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个ipython notebook主要是我解决Kaggle Titanic问题的思路和过程\n",
    "\n",
    "import pandas as pd #数据分析\n",
    "import numpy as np #科学计算\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data_train = pd.read_csv(\"./data/Train.csv\")\n",
    "data_train.columns\n",
    "#data_train[data_train.Cabin.notnull()]['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>我们看大概有以下这些字段</font>**<br>\n",
    "PassengerId => 乘客ID<br>\n",
    "Pclass => 乘客等级(1/2/3等舱位)<br>\n",
    "Name => 乘客姓名<br>\n",
    "Sex => 性别<br>\n",
    "Age => 年龄<br>\n",
    "SibSp => 堂兄弟/妹个数<br>\n",
    "Parch => 父母与小孩个数<br>\n",
    "Ticket => 船票信息<br>\n",
    "Fare => 票价<br>\n",
    "Cabin => 客舱<br>\n",
    "Embarked => 登船港口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>我这么懒的人显然会让pandas自己先告诉我们一些信息<font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>上面的数据说啥了？它告诉我们，训练数据中总共有891名乘客，但是很不幸，我们有些属性的数据不全，比如说：<font><br>\n",
    "\n",
    "* <font color=red>Age（年龄）属性只有714名乘客有记录<font>\n",
    "* <font color=red>Cabin（客舱）更是只有204名乘客是已知的<font>\n",
    "\n",
    "<font color=red>似乎信息略少啊，想再瞄一眼具体数据数值情况呢？恩，我们用下列的方法，得到数值型数据的一些分布(因为有些属性，比如姓名，是文本型；而另外一些属性，比如登船港口，是类目型。这些我们用下面的函数是看不到的)<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>mean字段告诉我们，大概0.383838的人最后获救了，2/3等舱的人数比1等舱要多，平均乘客年龄大概是29.7岁(计算这个时候会略掉无记录的)等等…<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "* <font color=red>『对数据的认识太重要了！』<font>\n",
    "\n",
    "<font color=red>口号喊完了，上面的简单描述信息并没有什么卵用啊，咱们得再细一点分析下数据啊。<font><br>\n",
    "<font color=red>看看**每个/多个 属性和最后的Survived**之间有着什么样的关系<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "plt.subplot2grid((2,3),(0,0))             # 在一张大图里分列几个小图\n",
    "data_train.Survived.value_counts().plot(kind='bar')# plots a bar graph of those who surived vs those who did not. \n",
    "plt.title(u\"获救情况 (1为获救)\") # puts a title on our graph\n",
    "plt.ylabel(u\"人数\")  \n",
    "\n",
    "plt.subplot2grid((2,3),(0,1))\n",
    "data_train.Pclass.value_counts().plot(kind=\"bar\")\n",
    "plt.ylabel(u\"人数\")\n",
    "plt.title(u\"乘客等级分布\")\n",
    "\n",
    "plt.subplot2grid((2,3),(0,2))\n",
    "plt.scatter(data_train.Survived, data_train.Age)\n",
    "plt.ylabel(u\"年龄\")                         # sets the y axis lable\n",
    "plt.grid(b=True, which='major', axis='y') # formats the grid line style of our graphs\n",
    "plt.title(u\"按年龄看获救分布 (1为获救)\")\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,0), colspan=2)\n",
    "data_train.Age[data_train.Pclass == 1].plot(kind='kde')   # plots a kernel desnsity estimate of the subset of the 1st class passanges's age\n",
    "data_train.Age[data_train.Pclass == 2].plot(kind='kde')\n",
    "data_train.Age[data_train.Pclass == 3].plot(kind='kde')\n",
    "plt.xlabel(u\"年龄\")# plots an axis lable\n",
    "plt.ylabel(u\"密度\") \n",
    "plt.title(u\"各等级的乘客年龄分布\")\n",
    "plt.legend((u'头等舱', u'2等舱',u'3等舱'),loc='best') # sets our legend for our graph.\n",
    "\n",
    "\n",
    "plt.subplot2grid((2,3),(1,2))\n",
    "data_train.Embarked.value_counts().plot(kind='bar')\n",
    "plt.title(u\"各登船口岸上船人数\")\n",
    "plt.ylabel(u\"人数\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是得到了像下面这样一张图：<br>\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>bingo，图还是比数字好看多了。所以我们在图上可以看出来:<font><br>\n",
    "* <font color=red>被救的人300多点，不到半数；<font><br>\n",
    "* <font color=red>3等舱乘客灰常多；遇难和获救的人年龄似乎跨度都很广；<font><br>\n",
    "* <font color=red>3个不同的舱年龄总体趋势似乎也一致，2/3等舱乘客20岁多点的人最多，1等舱40岁左右的最多(→_→似乎符合财富和年龄的分配哈，咳咳，别理我，我瞎扯的)；<font><br>\n",
    "* <font color=red>登船港口人数按照S、C、Q递减，而且S远多于另外俩港口。<font><br><br>\n",
    "\n",
    "<font color=red>这个时候我们可能会有一些想法了：<font><br><br>\n",
    "\n",
    "1. <font color=red>不同舱位/乘客等级可能和财富/地位有关系，最后获救概率可能会不一样<font><br>\n",
    "2. <font color=red>年龄对获救概率也一定是有影响的，毕竟前面说了，副船长还说『小孩和女士先走』呢<font><br>\n",
    "3. <font color=red>和登船港口是不是有关系呢？也许登船港口不同，人的出身地位不同？<font><br>\n",
    "\n",
    "<font color=red>口说无凭，空想无益。老老实实再来统计统计，看看这些属性值的统计分布吧。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21508 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20056 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 23458 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 31561 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32423 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33719 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25937 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24773 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20917 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20056 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 23458 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 31561 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32423 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21508 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33719 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25937 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24773 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20917 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26410 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26410 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各乘客等级的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_0 = data_train.Pclass[data_train.Survived == 0].value_counts()\n",
    "Survived_1 = data_train.Pclass[data_train.Survived == 1].value_counts()\n",
    "df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"各乘客等级的获救情况\")\n",
    "plt.xlabel(u\"乘客等级\") \n",
    "plt.ylabel(u\"人数\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>得到这个图：<font><br>\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/9.png?imageView/2/w/450/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>啧啧，果然，钱和地位对舱位有影响，进而对获救的可能性也有影响啊←_← <font><br>\n",
    "<font color=red>咳咳，跑题了，我想说的是，明显等级为1的乘客，获救的概率高很多。恩，这个一定是影响最后获救结果的一个特征。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30331 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24405 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 28207 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21475 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30331 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24405 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 28207 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21475 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各登录港口的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_0 = data_train.Embarked[data_train.Survived == 0].value_counts()\n",
    "Survived_1 = data_train.Embarked[data_train.Survived == 1].value_counts()\n",
    "df=pd.DataFrame({u'获救':Survived_1, u'未获救':Survived_0})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"各登录港口乘客的获救情况\")\n",
    "plt.xlabel(u\"登录港口\") \n",
    "plt.ylabel(u\"人数\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/Embarked.png?imageView/2/w/500/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>并没有看出什么...<font><br>\n",
    "\n",
    "<font color=red>那个，看看性别好了<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25353 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24615 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21035 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30475 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24615 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21035 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25353 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30475 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30007 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22899 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30007 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22899 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#看看各性别的获救情况\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_m = data_train.Survived[data_train.Sex == 'male'].value_counts()\n",
    "Survived_f = data_train.Survived[data_train.Sex == 'female'].value_counts()\n",
    "df=pd.DataFrame({u'男性':Survived_m, u'女性':Survived_f})\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"按性别看获救情况\")\n",
    "plt.xlabel(u\"性别\") \n",
    "plt.ylabel(u\"人数\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/10.png?imageView/2/w/450/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>歪果盆友果然很尊重lady，lady first践行得不错。性别无疑也要作为重要特征加入最后的模型之中。<font><br>\n",
    "\n",
    "<font color=red>再来个详细版的好了<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26681 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25454 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33329 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 31561 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 32423 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21644 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24615 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 21035 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 33719 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25937 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 24773 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20917 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26681 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25454 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33329 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 31561 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 32423 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21644 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24615 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 21035 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30340 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 33719 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25937 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 24773 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20917 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26410 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26410 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 22899 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 39640 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 22899 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 39640 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20302 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20302 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30007 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30007 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#然后我们再来看看各种舱级别情况下各性别的获救情况\n",
    "fig=plt.figure()\n",
    "fig.set(alpha=0.65) # 设置图像透明度，无所谓\n",
    "plt.title(u\"根据舱等级和性别的获救情况\")\n",
    "\n",
    "ax1=fig.add_subplot(141)\n",
    "data_train.Survived[data_train.Sex == 'female'][data_train.Pclass != 3].value_counts().plot(kind='bar', label=\"female highclass\", color='#FA2479')\n",
    "ax1.set_xticklabels([u\"获救\", u\"未获救\"], rotation=0)\n",
    "ax1.legend([u\"女性/高级舱\"], loc='best')\n",
    "\n",
    "ax2=fig.add_subplot(142, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'female'][data_train.Pclass == 3].value_counts().plot(kind='bar', label='female, low class', color='pink')\n",
    "ax2.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"女性/低级舱\"], loc='best')\n",
    "\n",
    "ax3=fig.add_subplot(143, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'male'][data_train.Pclass != 3].value_counts().plot(kind='bar', label='male, high class',color='lightblue')\n",
    "ax3.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"男性/高级舱\"], loc='best')\n",
    "\n",
    "ax4=fig.add_subplot(144, sharey=ax1)\n",
    "data_train.Survived[data_train.Sex == 'male'][data_train.Pclass == 3].value_counts().plot(kind='bar', label='male low class', color='steelblue')\n",
    "ax4.set_xticklabels([u\"未获救\", u\"获救\"], rotation=0)\n",
    "plt.legend([u\"男性/低级舱\"], loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/11.png?imageView/2/w/700/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>那堂兄弟和父母呢？<font>\n",
    "<font color=red>大家族会有优势么？<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PassengerId\n",
       "SibSp Survived             \n",
       "0     0                 398\n",
       "      1                 210\n",
       "1     0                  97\n",
       "      1                 112\n",
       "2     0                  15\n",
       "      1                  13\n",
       "3     0                  12\n",
       "      1                   4\n",
       "4     0                  15\n",
       "      1                   3\n",
       "5     0                   5\n",
       "8     0                   7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = data_train.groupby(['SibSp','Survived'])\n",
    "df = pd.DataFrame(g.count()['PassengerId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PassengerId\n",
       "Parch Survived             \n",
       "0     0                 445\n",
       "      1                 233\n",
       "1     0                  53\n",
       "      1                  65\n",
       "2     0                  40\n",
       "      1                  40\n",
       "3     0                   2\n",
       "      1                   3\n",
       "4     0                   4\n",
       "5     0                   4\n",
       "      1                   1\n",
       "6     0                   1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = data_train.groupby(['Parch','Survived'])\n",
    "df = pd.DataFrame(g.count()['PassengerId'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>好吧，没看出特别特别明显的规律(为自己的智商感到捉急…)，先作为备选特征，放一放。<font><br>\n",
    "<font color=red>看看船票好了<font><br>\n",
    "<font color=red>ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，不纳入考虑的特征范畴<font><br>\n",
    "<font color=red>cabin只有204个乘客有值，我们先看看它的一个分布<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B96 B98        4\n",
       "C23 C25 C27    4\n",
       "G6             4\n",
       "C22 C26        3\n",
       "F2             3\n",
       "E101           3\n",
       "D              3\n",
       "F33            3\n",
       "D36            2\n",
       "E121           2\n",
       "C78            2\n",
       "E24            2\n",
       "C2             2\n",
       "C68            2\n",
       "B28            2\n",
       "B51 B53 B55    2\n",
       "C124           2\n",
       "C52            2\n",
       "F4             2\n",
       "D17            2\n",
       "E25            2\n",
       "E8             2\n",
       "C92            2\n",
       "E44            2\n",
       "B49            2\n",
       "B18            2\n",
       "D35            2\n",
       "D33            2\n",
       "C65            2\n",
       "B58 B60        2\n",
       "              ..\n",
       "B42            1\n",
       "B50            1\n",
       "A14            1\n",
       "B4             1\n",
       "B78            1\n",
       "C95            1\n",
       "C70            1\n",
       "C46            1\n",
       "A7             1\n",
       "E40            1\n",
       "C82            1\n",
       "A16            1\n",
       "D11            1\n",
       "C49            1\n",
       "C85            1\n",
       "B94            1\n",
       "C62 C64        1\n",
       "A10            1\n",
       "B30            1\n",
       "A19            1\n",
       "D10 D12        1\n",
       "B71            1\n",
       "D6             1\n",
       "C104           1\n",
       "D19            1\n",
       "B39            1\n",
       "D28            1\n",
       "A36            1\n",
       "F G63          1\n",
       "T              1\n",
       "Name: Cabin, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ticket是船票编号，应该是unique的，和最后的结果没有太大的关系，不纳入考虑的特征范畴\n",
    "#cabin只有204个乘客有值，我们先看看它的一个分布\n",
    "data_train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>这三三两两的…如此不集中…我们猜一下，也许，前面的ABCDE是指的甲板位置、然后编号是房间号？…好吧，我瞎说的，别当真…<font><br>\n",
    "<font color=red>关键是Cabin这鬼属性，应该算作类目型的，本来缺失值就多，还如此不集中，注定是个棘手货…第一感觉，这玩意儿如果直接按照类目特征处理的话，太散了，估计每个因子化后的特征都拿不到什么权重。加上有那么多缺失值，要不我们先把Cabin缺失与否作为条件(虽然这部分信息缺失可能并非未登记，maybe只是丢失了而已，所以这样做未必妥当)，先在有无Cabin信息这个粗粒度上看看Survived的情况好了。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25353 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 26080 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 30475 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:176: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:176: RuntimeWarning: Glyph 26080 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26377 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 26080 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 20154 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 25353 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "e:\\liaopython\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 30475 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cabin的值计数太分散了，绝大多数Cabin值只出现一次。感觉上作为类目，加入特征未必会有效\n",
    "#那我们一起看看这个值的有无，对于survival的分布状况，影响如何吧\n",
    "fig = plt.figure()\n",
    "fig.set(alpha=0.2)  # 设定图表颜色alpha参数\n",
    "\n",
    "Survived_cabin = data_train.Survived[pd.notnull(data_train.Cabin)].value_counts()\n",
    "Survived_nocabin = data_train.Survived[pd.isnull(data_train.Cabin)].value_counts()\n",
    "df=pd.DataFrame({u'有':Survived_cabin, u'无':Survived_nocabin}).transpose()\n",
    "df.plot(kind='bar', stacked=True)\n",
    "plt.title(u\"按Cabin有无看获救情况\")\n",
    "plt.xlabel(u\"Cabin有无\") \n",
    "plt.ylabel(u\"人数\")\n",
    "plt.show()\n",
    "\n",
    "#似乎有cabin记录的乘客survival比例稍高，那先试试把这个值分为两类，有cabin值/无cabin值，一会儿加到类别特征好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/15.png?imageView/2/w/400/q/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>有Cabin记录的似乎获救概率稍高一些，先这么着放一放吧。<font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>先从最突出的数据属性开始吧，对，Cabin和Age，有丢失数据实在是对下一步工作影响太大。<font><br>\n",
    "\n",
    "<font color=red>先说Cabin，暂时我们就按照刚才说的，按Cabin有无数据，将这个属性处理成Yes和No两种类型吧。<font><br>\n",
    "\n",
    "<font color=red>再说Age：<font><br>\n",
    "\n",
    "<font color=red>通常遇到缺值的情况，我们会有几种常见的处理方式<font><br>\n",
    "\n",
    "1. <font color=red>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了<font><br>\n",
    "2. <font color=red>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中<font><br>\n",
    "3. <font color=red>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。<font><br>\n",
    "4. <font color=red>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。<font><br>\n",
    "<font color=red>本例中，后两种处理方式应该都是可行的，我们先试试拟合补全吧(虽然说没有特别多的背景可供我们拟合，这不一定是一个多么好的选择)<font><br>\n",
    "\n",
    "<font color=red>我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\liaopython\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "e:\\liaopython\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>23.838953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>32.066493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>22.380113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Giles, Mr. Frederick Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28134</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17466</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>10.869867</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Gill, Mr. John William</td>\n",
       "      <td>male</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233866</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Duran y More, Miss. Asuncion</td>\n",
       "      <td>female</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2149</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>25.977889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345777</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Master. Harold Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Balkic, Mr. Cerin</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349248</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Abelson, Mrs. Samuel (Hannah Wizosky)</td>\n",
       "      <td>female</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P/PP 3381</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Najib, Miss. Adele Kiamie \"Jane\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2667</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>No</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Nedelio</td>\n",
       "      <td>male</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349212</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349217</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>female</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Shelley, Mrs. William (Imanita Parrish Hall)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230433</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Markun, Mr. Johann</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349257</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dahlberg, Miss. Gerda Ulrika</td>\n",
       "      <td>female</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Banfield, Mr. Frederick James</td>\n",
       "      <td>male</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A./SOTON 34068</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sutehall, Mr. Henry Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392076</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Mrs. William (Margaret Norton)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>16.193950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>No</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>No</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "5              6         0       3   \n",
       "6              7         0       1   \n",
       "7              8         0       3   \n",
       "8              9         1       3   \n",
       "9             10         1       2   \n",
       "10            11         1       3   \n",
       "11            12         1       1   \n",
       "12            13         0       3   \n",
       "13            14         0       3   \n",
       "14            15         0       3   \n",
       "15            16         1       2   \n",
       "16            17         0       3   \n",
       "17            18         1       2   \n",
       "18            19         0       3   \n",
       "19            20         1       3   \n",
       "20            21         0       2   \n",
       "21            22         1       2   \n",
       "22            23         1       3   \n",
       "23            24         1       1   \n",
       "24            25         0       3   \n",
       "25            26         1       3   \n",
       "26            27         0       3   \n",
       "27            28         0       1   \n",
       "28            29         1       3   \n",
       "29            30         0       3   \n",
       "..           ...       ...     ...   \n",
       "861          862         0       2   \n",
       "862          863         1       1   \n",
       "863          864         0       3   \n",
       "864          865         0       2   \n",
       "865          866         1       2   \n",
       "866          867         1       2   \n",
       "867          868         0       1   \n",
       "868          869         0       3   \n",
       "869          870         1       3   \n",
       "870          871         0       3   \n",
       "871          872         1       1   \n",
       "872          873         0       1   \n",
       "873          874         0       3   \n",
       "874          875         1       2   \n",
       "875          876         1       3   \n",
       "876          877         0       3   \n",
       "877          878         0       3   \n",
       "878          879         0       3   \n",
       "879          880         1       1   \n",
       "880          881         1       2   \n",
       "881          882         0       3   \n",
       "882          883         0       3   \n",
       "883          884         0       2   \n",
       "884          885         0       3   \n",
       "885          886         0       3   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex        Age  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.000000   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
       "2                               Heikkinen, Miss. Laina  female  26.000000   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
       "4                             Allen, Mr. William Henry    male  35.000000   \n",
       "5                                     Moran, Mr. James    male  23.838953   \n",
       "6                              McCarthy, Mr. Timothy J    male  54.000000   \n",
       "7                       Palsson, Master. Gosta Leonard    male   2.000000   \n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.000000   \n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.000000   \n",
       "10                     Sandstrom, Miss. Marguerite Rut  female   4.000000   \n",
       "11                            Bonnell, Miss. Elizabeth  female  58.000000   \n",
       "12                      Saundercock, Mr. William Henry    male  20.000000   \n",
       "13                         Andersson, Mr. Anders Johan    male  39.000000   \n",
       "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.000000   \n",
       "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.000000   \n",
       "16                                Rice, Master. Eugene    male   2.000000   \n",
       "17                        Williams, Mr. Charles Eugene    male  32.066493   \n",
       "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.000000   \n",
       "19                             Masselmani, Mrs. Fatima  female  29.518205   \n",
       "20                                Fynney, Mr. Joseph J    male  35.000000   \n",
       "21                               Beesley, Mr. Lawrence    male  34.000000   \n",
       "22                         McGowan, Miss. Anna \"Annie\"  female  15.000000   \n",
       "23                        Sloper, Mr. William Thompson    male  28.000000   \n",
       "24                       Palsson, Miss. Torborg Danira  female   8.000000   \n",
       "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.000000   \n",
       "26                             Emir, Mr. Farred Chehab    male  29.518205   \n",
       "27                      Fortune, Mr. Charles Alexander    male  19.000000   \n",
       "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female  22.380113   \n",
       "29                                 Todoroff, Mr. Lalio    male  27.947206   \n",
       "..                                                 ...     ...        ...   \n",
       "861                        Giles, Mr. Frederick Edward    male  21.000000   \n",
       "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.000000   \n",
       "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female  10.869867   \n",
       "864                             Gill, Mr. John William    male  24.000000   \n",
       "865                           Bystrom, Mrs. (Karolina)  female  42.000000   \n",
       "866                       Duran y More, Miss. Asuncion  female  27.000000   \n",
       "867               Roebling, Mr. Washington Augustus II    male  31.000000   \n",
       "868                        van Melkebeke, Mr. Philemon    male  25.977889   \n",
       "869                    Johnson, Master. Harold Theodor    male   4.000000   \n",
       "870                                  Balkic, Mr. Cerin    male  26.000000   \n",
       "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.000000   \n",
       "872                           Carlsson, Mr. Frans Olof    male  33.000000   \n",
       "873                        Vander Cruyssen, Mr. Victor    male  47.000000   \n",
       "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.000000   \n",
       "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.000000   \n",
       "876                      Gustafsson, Mr. Alfred Ossian    male  20.000000   \n",
       "877                               Petroff, Mr. Nedelio    male  19.000000   \n",
       "878                                 Laleff, Mr. Kristo    male  27.947206   \n",
       "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.000000   \n",
       "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.000000   \n",
       "881                                 Markun, Mr. Johann    male  33.000000   \n",
       "882                       Dahlberg, Miss. Gerda Ulrika  female  22.000000   \n",
       "883                      Banfield, Mr. Frederick James    male  28.000000   \n",
       "884                             Sutehall, Mr. Henry Jr    male  25.000000   \n",
       "885               Rice, Mrs. William (Margaret Norton)  female  39.000000   \n",
       "886                              Montvila, Rev. Juozas    male  27.000000   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.000000   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  16.193950   \n",
       "889                              Behr, Mr. Karl Howell    male  26.000000   \n",
       "890                                Dooley, Mr. Patrick    male  32.000000   \n",
       "\n",
       "     SibSp  Parch            Ticket      Fare Cabin Embarked  \n",
       "0        1      0         A/5 21171    7.2500    No        S  \n",
       "1        1      0          PC 17599   71.2833   Yes        C  \n",
       "2        0      0  STON/O2. 3101282    7.9250    No        S  \n",
       "3        1      0            113803   53.1000   Yes        S  \n",
       "4        0      0            373450    8.0500    No        S  \n",
       "5        0      0            330877    8.4583    No        Q  \n",
       "6        0      0             17463   51.8625   Yes        S  \n",
       "7        3      1            349909   21.0750    No        S  \n",
       "8        0      2            347742   11.1333    No        S  \n",
       "9        1      0            237736   30.0708    No        C  \n",
       "10       1      1           PP 9549   16.7000   Yes        S  \n",
       "11       0      0            113783   26.5500   Yes        S  \n",
       "12       0      0         A/5. 2151    8.0500    No        S  \n",
       "13       1      5            347082   31.2750    No        S  \n",
       "14       0      0            350406    7.8542    No        S  \n",
       "15       0      0            248706   16.0000    No        S  \n",
       "16       4      1            382652   29.1250    No        Q  \n",
       "17       0      0            244373   13.0000    No        S  \n",
       "18       1      0            345763   18.0000    No        S  \n",
       "19       0      0              2649    7.2250    No        C  \n",
       "20       0      0            239865   26.0000    No        S  \n",
       "21       0      0            248698   13.0000   Yes        S  \n",
       "22       0      0            330923    8.0292    No        Q  \n",
       "23       0      0            113788   35.5000   Yes        S  \n",
       "24       3      1            349909   21.0750    No        S  \n",
       "25       1      5            347077   31.3875    No        S  \n",
       "26       0      0              2631    7.2250    No        C  \n",
       "27       3      2             19950  263.0000   Yes        S  \n",
       "28       0      0            330959    7.8792    No        Q  \n",
       "29       0      0            349216    7.8958    No        S  \n",
       "..     ...    ...               ...       ...   ...      ...  \n",
       "861      1      0             28134   11.5000    No        S  \n",
       "862      0      0             17466   25.9292   Yes        S  \n",
       "863      8      2          CA. 2343   69.5500    No        S  \n",
       "864      0      0            233866   13.0000    No        S  \n",
       "865      0      0            236852   13.0000    No        S  \n",
       "866      1      0     SC/PARIS 2149   13.8583    No        C  \n",
       "867      0      0          PC 17590   50.4958   Yes        S  \n",
       "868      0      0            345777    9.5000    No        S  \n",
       "869      1      1            347742   11.1333    No        S  \n",
       "870      0      0            349248    7.8958    No        S  \n",
       "871      1      1             11751   52.5542   Yes        S  \n",
       "872      0      0               695    5.0000   Yes        S  \n",
       "873      0      0            345765    9.0000    No        S  \n",
       "874      1      0         P/PP 3381   24.0000    No        C  \n",
       "875      0      0              2667    7.2250    No        C  \n",
       "876      0      0              7534    9.8458    No        S  \n",
       "877      0      0            349212    7.8958    No        S  \n",
       "878      0      0            349217    7.8958    No        S  \n",
       "879      0      1             11767   83.1583   Yes        C  \n",
       "880      0      1            230433   26.0000    No        S  \n",
       "881      0      0            349257    7.8958    No        S  \n",
       "882      0      0              7552   10.5167    No        S  \n",
       "883      0      0  C.A./SOTON 34068   10.5000    No        S  \n",
       "884      0      0   SOTON/OQ 392076    7.0500    No        S  \n",
       "885      0      5            382652   29.1250    No        Q  \n",
       "886      0      0            211536   13.0000    No        S  \n",
       "887      0      0            112053   30.0000   Yes        S  \n",
       "888      1      2        W./C. 6607   23.4500    No        S  \n",
       "889      0      0            111369   30.0000   Yes        C  \n",
       "890      0      0            370376    7.7500    No        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先对类目型的特征因子化/one-hot编码。 <font><br>\n",
    "<font color=red>什么叫做因子化/one-hot编码？举个例子：<font><br>\n",
    "\n",
    "<font color=red>以Embarked为例，原本一个属性维度，因为其取值可以是[‘S’,’C’,’Q‘]，而将其平展开为’Embarked_C’,’Embarked_S’, ‘Embarked_Q’三个属性<font><br>\n",
    "\n",
    "* <font color=red>原本Embarked取值为S的，在此处的”Embarked_S”下取值为1，在’Embarked_C’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=red>原本Embarked取值为C的，在此处的”Embarked_C”下取值为1，在’Embarked_S’, ‘Embarked_Q’下取值为0<font><br>\n",
    "* <font color=red>原本Embarked取值为Q的，在此处的”Embarked_Q”下取值为1，在’Embarked_C’, ‘Embarked_S’下取值为0<font><br>\n",
    "\n",
    "<font color=red>我们使用pandas的”get_dummies”来完成这个工作，并拼接在原来的”data_train”之上，如下所示。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin_No</th>\n",
       "      <th>Cabin_Yes</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23.838953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>32.066493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>29.518205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>22.380113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>10.869867</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>25.977889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>27.947206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>16.193950</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived        Age  SibSp  Parch      Fare  Cabin_No  \\\n",
       "0              1         0  22.000000      1      0    7.2500         1   \n",
       "1              2         1  38.000000      1      0   71.2833         0   \n",
       "2              3         1  26.000000      0      0    7.9250         1   \n",
       "3              4         1  35.000000      1      0   53.1000         0   \n",
       "4              5         0  35.000000      0      0    8.0500         1   \n",
       "5              6         0  23.838953      0      0    8.4583         1   \n",
       "6              7         0  54.000000      0      0   51.8625         0   \n",
       "7              8         0   2.000000      3      1   21.0750         1   \n",
       "8              9         1  27.000000      0      2   11.1333         1   \n",
       "9             10         1  14.000000      1      0   30.0708         1   \n",
       "10            11         1   4.000000      1      1   16.7000         0   \n",
       "11            12         1  58.000000      0      0   26.5500         0   \n",
       "12            13         0  20.000000      0      0    8.0500         1   \n",
       "13            14         0  39.000000      1      5   31.2750         1   \n",
       "14            15         0  14.000000      0      0    7.8542         1   \n",
       "15            16         1  55.000000      0      0   16.0000         1   \n",
       "16            17         0   2.000000      4      1   29.1250         1   \n",
       "17            18         1  32.066493      0      0   13.0000         1   \n",
       "18            19         0  31.000000      1      0   18.0000         1   \n",
       "19            20         1  29.518205      0      0    7.2250         1   \n",
       "20            21         0  35.000000      0      0   26.0000         1   \n",
       "21            22         1  34.000000      0      0   13.0000         0   \n",
       "22            23         1  15.000000      0      0    8.0292         1   \n",
       "23            24         1  28.000000      0      0   35.5000         0   \n",
       "24            25         0   8.000000      3      1   21.0750         1   \n",
       "25            26         1  38.000000      1      5   31.3875         1   \n",
       "26            27         0  29.518205      0      0    7.2250         1   \n",
       "27            28         0  19.000000      3      2  263.0000         0   \n",
       "28            29         1  22.380113      0      0    7.8792         1   \n",
       "29            30         0  27.947206      0      0    7.8958         1   \n",
       "..           ...       ...        ...    ...    ...       ...       ...   \n",
       "861          862         0  21.000000      1      0   11.5000         1   \n",
       "862          863         1  48.000000      0      0   25.9292         0   \n",
       "863          864         0  10.869867      8      2   69.5500         1   \n",
       "864          865         0  24.000000      0      0   13.0000         1   \n",
       "865          866         1  42.000000      0      0   13.0000         1   \n",
       "866          867         1  27.000000      1      0   13.8583         1   \n",
       "867          868         0  31.000000      0      0   50.4958         0   \n",
       "868          869         0  25.977889      0      0    9.5000         1   \n",
       "869          870         1   4.000000      1      1   11.1333         1   \n",
       "870          871         0  26.000000      0      0    7.8958         1   \n",
       "871          872         1  47.000000      1      1   52.5542         0   \n",
       "872          873         0  33.000000      0      0    5.0000         0   \n",
       "873          874         0  47.000000      0      0    9.0000         1   \n",
       "874          875         1  28.000000      1      0   24.0000         1   \n",
       "875          876         1  15.000000      0      0    7.2250         1   \n",
       "876          877         0  20.000000      0      0    9.8458         1   \n",
       "877          878         0  19.000000      0      0    7.8958         1   \n",
       "878          879         0  27.947206      0      0    7.8958         1   \n",
       "879          880         1  56.000000      0      1   83.1583         0   \n",
       "880          881         1  25.000000      0      1   26.0000         1   \n",
       "881          882         0  33.000000      0      0    7.8958         1   \n",
       "882          883         0  22.000000      0      0   10.5167         1   \n",
       "883          884         0  28.000000      0      0   10.5000         1   \n",
       "884          885         0  25.000000      0      0    7.0500         1   \n",
       "885          886         0  39.000000      0      5   29.1250         1   \n",
       "886          887         0  27.000000      0      0   13.0000         1   \n",
       "887          888         1  19.000000      0      0   30.0000         0   \n",
       "888          889         0  16.193950      1      2   23.4500         1   \n",
       "889          890         1  26.000000      0      0   30.0000         0   \n",
       "890          891         0  32.000000      0      0    7.7500         1   \n",
       "\n",
       "     Cabin_Yes  Embarked_C  Embarked_Q  Embarked_S  Sex_female  Sex_male  \\\n",
       "0            0           0           0           1           0         1   \n",
       "1            1           1           0           0           1         0   \n",
       "2            0           0           0           1           1         0   \n",
       "3            1           0           0           1           1         0   \n",
       "4            0           0           0           1           0         1   \n",
       "5            0           0           1           0           0         1   \n",
       "6            1           0           0           1           0         1   \n",
       "7            0           0           0           1           0         1   \n",
       "8            0           0           0           1           1         0   \n",
       "9            0           1           0           0           1         0   \n",
       "10           1           0           0           1           1         0   \n",
       "11           1           0           0           1           1         0   \n",
       "12           0           0           0           1           0         1   \n",
       "13           0           0           0           1           0         1   \n",
       "14           0           0           0           1           1         0   \n",
       "15           0           0           0           1           1         0   \n",
       "16           0           0           1           0           0         1   \n",
       "17           0           0           0           1           0         1   \n",
       "18           0           0           0           1           1         0   \n",
       "19           0           1           0           0           1         0   \n",
       "20           0           0           0           1           0         1   \n",
       "21           1           0           0           1           0         1   \n",
       "22           0           0           1           0           1         0   \n",
       "23           1           0           0           1           0         1   \n",
       "24           0           0           0           1           1         0   \n",
       "25           0           0           0           1           1         0   \n",
       "26           0           1           0           0           0         1   \n",
       "27           1           0           0           1           0         1   \n",
       "28           0           0           1           0           1         0   \n",
       "29           0           0           0           1           0         1   \n",
       "..         ...         ...         ...         ...         ...       ...   \n",
       "861          0           0           0           1           0         1   \n",
       "862          1           0           0           1           1         0   \n",
       "863          0           0           0           1           1         0   \n",
       "864          0           0           0           1           0         1   \n",
       "865          0           0           0           1           1         0   \n",
       "866          0           1           0           0           1         0   \n",
       "867          1           0           0           1           0         1   \n",
       "868          0           0           0           1           0         1   \n",
       "869          0           0           0           1           0         1   \n",
       "870          0           0           0           1           0         1   \n",
       "871          1           0           0           1           1         0   \n",
       "872          1           0           0           1           0         1   \n",
       "873          0           0           0           1           0         1   \n",
       "874          0           1           0           0           1         0   \n",
       "875          0           1           0           0           1         0   \n",
       "876          0           0           0           1           0         1   \n",
       "877          0           0           0           1           0         1   \n",
       "878          0           0           0           1           0         1   \n",
       "879          1           1           0           0           1         0   \n",
       "880          0           0           0           1           1         0   \n",
       "881          0           0           0           1           0         1   \n",
       "882          0           0           0           1           1         0   \n",
       "883          0           0           0           1           0         1   \n",
       "884          0           0           0           1           0         1   \n",
       "885          0           0           1           0           1         0   \n",
       "886          0           0           0           1           0         1   \n",
       "887          1           0           0           1           1         0   \n",
       "888          0           0           0           1           1         0   \n",
       "889          1           1           0           0           0         1   \n",
       "890          0           0           1           0           0         1   \n",
       "\n",
       "     Pclass_1  Pclass_2  Pclass_3  \n",
       "0           0         0         1  \n",
       "1           1         0         0  \n",
       "2           0         0         1  \n",
       "3           1         0         0  \n",
       "4           0         0         1  \n",
       "5           0         0         1  \n",
       "6           1         0         0  \n",
       "7           0         0         1  \n",
       "8           0         0         1  \n",
       "9           0         1         0  \n",
       "10          0         0         1  \n",
       "11          1         0         0  \n",
       "12          0         0         1  \n",
       "13          0         0         1  \n",
       "14          0         0         1  \n",
       "15          0         1         0  \n",
       "16          0         0         1  \n",
       "17          0         1         0  \n",
       "18          0         0         1  \n",
       "19          0         0         1  \n",
       "20          0         1         0  \n",
       "21          0         1         0  \n",
       "22          0         0         1  \n",
       "23          1         0         0  \n",
       "24          0         0         1  \n",
       "25          0         0         1  \n",
       "26          0         0         1  \n",
       "27          1         0         0  \n",
       "28          0         0         1  \n",
       "29          0         0         1  \n",
       "..        ...       ...       ...  \n",
       "861         0         1         0  \n",
       "862         1         0         0  \n",
       "863         0         0         1  \n",
       "864         0         1         0  \n",
       "865         0         1         0  \n",
       "866         0         1         0  \n",
       "867         1         0         0  \n",
       "868         0         0         1  \n",
       "869         0         0         1  \n",
       "870         0         0         1  \n",
       "871         1         0         0  \n",
       "872         1         0         0  \n",
       "873         0         0         1  \n",
       "874         0         1         0  \n",
       "875         0         0         1  \n",
       "876         0         0         1  \n",
       "877         0         0         1  \n",
       "878         0         0         1  \n",
       "879         1         0         0  \n",
       "880         0         1         0  \n",
       "881         0         0         1  \n",
       "882         0         0         1  \n",
       "883         0         1         0  \n",
       "884         0         0         1  \n",
       "885         0         0         1  \n",
       "886         0         1         0  \n",
       "887         1         0         0  \n",
       "888         0         0         1  \n",
       "889         1         0         0  \n",
       "890         0         0         1  \n",
       "\n",
       "[891 rows x 16 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为逻辑回归建模时，需要输入的特征都是数值型特征\n",
    "# 我们先对类目型的特征离散/因子化\n",
    "# 以Cabin为例，原本一个属性维度，因为其取值可以是['yes','no']，而将其平展开为'Cabin_yes','Cabin_no'两个属性\n",
    "# 原本Cabin取值为yes的，在此处的'Cabin_yes'下取值为1，在'Cabin_no'下取值为0\n",
    "# 原本Cabin取值为no的，在此处的'Cabin_yes'下取值为0，在'Cabin_no'下取值为1\n",
    "# 我们使用pandas的get_dummies来完成这个工作，并拼接在原来的data_train之上，如下所示\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>我们还得做一些处理，仔细看看Age和Fare两个属性，乘客的数值幅度变化，也忒大了吧！！如果大家了解逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！ (╬▔皿▔)…所以我们先用scikit-learn里面的preprocessing模块对这俩货做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[22.         38.         26.         35.         35.         23.83895259\n 54.          2.         27.         14.          4.         58.\n 20.         39.         14.         55.          2.         32.06649305\n 31.         29.51820514 35.         34.         15.         28.\n  8.         38.         29.51820514 19.         22.38011324 27.94720616\n 40.         36.10804822 35.2958243  66.         28.         42.\n 22.87630686 21.         18.         14.         40.         27.\n 27.94720616  3.         19.         30.70572678 33.12898535 35.2958243\n 23.45968333 18.          7.         21.         49.         29.\n 65.         44.06483036 21.         28.5         5.         11.\n 22.         38.         45.          4.         41.20008848 17.09991595\n 29.         19.         17.         26.         32.         16.\n 21.         26.         32.         25.         27.94720616 30.70572678\n  0.83       30.         22.         29.         23.32262739 28.\n 17.         33.         16.         30.70572678 23.         24.\n 29.         20.         46.         26.         59.         30.70572678\n 71.         23.         34.         34.         28.         27.94720616\n 21.         33.         37.         28.         21.         27.51545426\n 38.         33.55117591 47.         14.5        22.         20.\n 17.         21.         70.5        29.         24.          2.\n 21.         30.70572678 32.5        32.5        54.         12.\n 35.2958243  24.         25.78337698 45.         33.         20.\n 47.         29.         25.         23.         19.         37.\n 16.         24.         25.34409583 22.         24.         19.\n 18.         19.         27.          9.         36.5        42.\n 51.         22.         55.5        40.5        29.78279613 51.\n 16.         30.         25.52340334 10.86986696 44.         40.\n 26.         17.          1.          9.         26.03188214 45.\n 49.5542756  28.         61.          4.          1.         21.\n 56.         18.          7.30954704 50.         30.         36.\n 10.86986696 31.71894048  9.          1.          4.         46.24976824\n 33.12898535 45.         40.         36.         32.         19.\n 19.          3.         44.         58.         35.2958243  42.\n 35.2958243  24.         28.         10.86986696 34.         45.5\n 18.          2.         32.         26.         16.         40.\n 24.         35.         22.         30.         31.09342985 31.\n 27.         42.         32.         30.         16.         27.\n 51.         27.94720616 38.         22.         19.         20.5\n 18.          7.30954704 35.         29.         59.          5.\n 24.         31.10838452 44.          8.         19.         33.\n 20.80015413 33.12898535 29.         22.         30.         44.\n 25.         24.         37.         54.         29.78279613 29.\n 62.         30.         41.         29.         34.62028571 30.\n 35.         50.         35.2958243   3.         52.         40.\n 35.2958243  36.         16.         25.         58.         35.\n 36.87489821 25.         41.         37.         35.2958243  63.\n 45.         35.05181757  7.         35.         65.         28.\n 16.         19.         57.74249226 33.         30.         22.\n 42.         22.         26.         19.         36.         24.\n 24.         41.20008848 23.5         2.         41.57487718 50.\n 35.2958243  23.31368333 19.         42.57451554 30.70572678  0.92\n 28.57888393 17.         30.         30.         24.         18.\n 26.         28.         43.         26.         24.         54.\n 31.         40.         22.         27.         30.         22.\n 10.86986696 36.         61.         36.         31.         16.\n 23.31368333 45.5        38.         16.         31.42587794 27.94720616\n 29.         41.         45.         45.          2.         24.\n 28.         25.         36.         24.         40.         26.68916849\n  3.         42.         23.         59.96916448 15.         25.\n 29.51820514 28.         22.         38.         22.38011324 22.38011324\n 40.         29.         45.         35.         33.12898535 30.\n 60.         22.87630686 35.2958243  24.         25.         18.\n 19.         22.          3.         31.94479345 22.         27.\n 20.         19.         42.          1.         32.         35.\n 27.94720616 18.          1.         36.         19.89558113 17.\n 36.         21.         28.         23.         24.         22.\n 31.         46.         23.         28.         39.         26.\n 21.         28.         20.         34.         51.          3.\n 21.          7.30954704 27.94720616 20.68131488 33.         35.05181757\n 44.         30.70572678 34.         18.         30.         10.\n 27.94720616 21.         29.         28.         18.         29.78279613\n 28.         19.         35.2958243  32.         28.         26.68916849\n 42.         17.         50.         14.         21.         24.\n 64.         31.         45.         20.         25.         28.\n 23.47643239  4.         13.         34.          5.         52.\n 36.         24.10899881 30.         49.         30.70572678 29.\n 65.         44.06001827 50.         35.2958243  48.         34.\n 47.         48.         30.70572678 38.         35.05181757 56.\n 19.89558113  0.75       29.78279613 38.         33.         23.\n 22.         44.65953056 34.         29.         22.          2.\n  9.         35.05181757 50.         63.         25.          7.30954704\n 35.         58.         30.          9.         24.10899881 21.\n 55.         71.         21.         18.19479484 54.         25.7554753\n 25.         24.         17.         21.         26.82073281 37.\n 16.         18.         33.         49.8994093  28.         26.\n 29.         30.70572678 36.         54.         24.         47.\n 34.         34.17374861 36.         32.         30.         22.\n 29.51820514 44.         22.87630686 40.5        50.         38.5155\n 39.         23.          2.         22.87630686 17.          8.01370298\n 30.          7.         45.         30.         24.33021696 22.\n 36.          9.         11.         32.         50.         64.\n 19.         32.79502428 33.          8.         17.         27.\n 26.3814249  22.         22.         62.         48.         38.5155\n 39.         36.         35.2958243  40.         28.         30.70572678\n 30.70572678 24.         19.         29.         22.87630686 32.\n 62.         53.         36.         35.2958243  16.         19.\n 34.         39.         20.07016773 32.         25.         39.\n 54.         36.         29.04293865 18.         47.         60.\n 22.         30.70572678 35.         52.         47.         27.45639428\n 37.         36.         24.40235514 49.         29.51820514 49.\n 24.         27.94720616 37.51615833 44.         35.         36.\n 30.         27.         22.         40.         39.         27.86873699\n 33.12898535 35.2958243  35.         24.         34.         26.\n  4.         26.         27.         42.         20.         21.\n 21.         61.         57.         21.         26.         19.89558113\n 80.         51.         32.         38.83477484  9.         28.\n 32.         31.         41.         26.68916849 20.         24.\n  2.         29.56889809  0.75       48.         19.         56.\n 31.10838452 23.         27.94720616 18.         21.         26.3814249\n 18.         24.         27.94720616 32.         23.         58.\n 50.         40.         47.         36.         20.         32.\n 25.         27.51545426 43.         39.08817814 40.         31.\n 70.         31.         35.05181757 18.         24.5        18.\n 43.         36.         23.47643239 27.         20.         14.\n 60.         25.         14.         19.         18.         15.\n 31.          4.         29.56889809 25.         60.         52.\n 44.         19.89558113 49.         42.         18.         35.\n 18.         25.         26.         39.         45.         42.\n 22.         17.09991595 24.         49.8994093  48.         29.\n 52.         19.         38.         27.         27.30887391 33.\n  6.         17.         34.         50.         27.         20.\n 30.         19.89558113 25.         25.         29.         11.\n 35.05181757 23.         23.         28.5        48.         35.\n 27.94720616 27.94720616 38.42663175 36.         21.         24.\n 31.         70.         16.         30.         19.         31.\n  4.          6.         33.         23.         48.          0.67\n 28.         18.         34.         33.         24.33021696 41.\n 20.         36.         16.         51.         39.17490238 30.5\n 33.55117591 32.         24.         48.         57.         29.51820514\n 54.         18.         35.2958243   5.         19.89558113 43.\n 13.         17.         29.         16.1939502  25.         25.\n 18.          8.          1.         46.         35.2958243  16.\n 10.86986696 50.38328427 25.         39.         49.         31.\n 30.         30.         34.         31.         11.          0.42\n 27.         31.         39.         18.         39.         33.\n 26.         39.         35.          6.         30.5        38.83477484\n 23.         31.         43.         10.         52.         27.\n 38.         27.          2.         35.09787898 29.56889809  1.\n 35.2958243  62.         15.          0.83       22.87630686 23.\n 18.         39.         21.         30.70572678 32.         50.91095013\n 20.         16.         30.         34.5        17.         42.\n 10.86986696 35.         28.         43.96476448  4.         74.\n  9.         16.         44.         18.         45.         51.\n 24.         22.87630686 41.         21.         48.         10.86986696\n 24.         42.         27.         31.         25.97788916  4.\n 26.         47.         33.         47.         28.         15.\n 20.         19.         27.94720616 56.         25.         33.\n 22.         28.         25.         39.         27.         19.\n 16.1939502  26.         32.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-24b6b3e8bee5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mage_scale_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age_scaled'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage_scale_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfare_scale_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fare'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\liaopython\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\liaopython\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    661\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m    662\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\liaopython\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[22.         38.         26.         35.         35.         23.83895259\n 54.          2.         27.         14.          4.         58.\n 20.         39.         14.         55.          2.         32.06649305\n 31.         29.51820514 35.         34.         15.         28.\n  8.         38.         29.51820514 19.         22.38011324 27.94720616\n 40.         36.10804822 35.2958243  66.         28.         42.\n 22.87630686 21.         18.         14.         40.         27.\n 27.94720616  3.         19.         30.70572678 33.12898535 35.2958243\n 23.45968333 18.          7.         21.         49.         29.\n 65.         44.06483036 21.         28.5         5.         11.\n 22.         38.         45.          4.         41.20008848 17.09991595\n 29.         19.         17.         26.         32.         16.\n 21.         26.         32.         25.         27.94720616 30.70572678\n  0.83       30.         22.         29.         23.32262739 28.\n 17.         33.         16.         30.70572678 23.         24.\n 29.         20.         46.         26.         59.         30.70572678\n 71.         23.         34.         34.         28.         27.94720616\n 21.         33.         37.         28.         21.         27.51545426\n 38.         33.55117591 47.         14.5        22.         20.\n 17.         21.         70.5        29.         24.          2.\n 21.         30.70572678 32.5        32.5        54.         12.\n 35.2958243  24.         25.78337698 45.         33.         20.\n 47.         29.         25.         23.         19.         37.\n 16.         24.         25.34409583 22.         24.         19.\n 18.         19.         27.          9.         36.5        42.\n 51.         22.         55.5        40.5        29.78279613 51.\n 16.         30.         25.52340334 10.86986696 44.         40.\n 26.         17.          1.          9.         26.03188214 45.\n 49.5542756  28.         61.          4.          1.         21.\n 56.         18.          7.30954704 50.         30.         36.\n 10.86986696 31.71894048  9.          1.          4.         46.24976824\n 33.12898535 45.         40.         36.         32.         19.\n 19.          3.         44.         58.         35.2958243  42.\n 35.2958243  24.         28.         10.86986696 34.         45.5\n 18.          2.         32.         26.         16.         40.\n 24.         35.         22.         30.         31.09342985 31.\n 27.         42.         32.         30.         16.         27.\n 51.         27.94720616 38.         22.         19.         20.5\n 18.          7.30954704 35.         29.         59.          5.\n 24.         31.10838452 44.          8.         19.         33.\n 20.80015413 33.12898535 29.         22.         30.         44.\n 25.         24.         37.         54.         29.78279613 29.\n 62.         30.         41.         29.         34.62028571 30.\n 35.         50.         35.2958243   3.         52.         40.\n 35.2958243  36.         16.         25.         58.         35.\n 36.87489821 25.         41.         37.         35.2958243  63.\n 45.         35.05181757  7.         35.         65.         28.\n 16.         19.         57.74249226 33.         30.         22.\n 42.         22.         26.         19.         36.         24.\n 24.         41.20008848 23.5         2.         41.57487718 50.\n 35.2958243  23.31368333 19.         42.57451554 30.70572678  0.92\n 28.57888393 17.         30.         30.         24.         18.\n 26.         28.         43.         26.         24.         54.\n 31.         40.         22.         27.         30.         22.\n 10.86986696 36.         61.         36.         31.         16.\n 23.31368333 45.5        38.         16.         31.42587794 27.94720616\n 29.         41.         45.         45.          2.         24.\n 28.         25.         36.         24.         40.         26.68916849\n  3.         42.         23.         59.96916448 15.         25.\n 29.51820514 28.         22.         38.         22.38011324 22.38011324\n 40.         29.         45.         35.         33.12898535 30.\n 60.         22.87630686 35.2958243  24.         25.         18.\n 19.         22.          3.         31.94479345 22.         27.\n 20.         19.         42.          1.         32.         35.\n 27.94720616 18.          1.         36.         19.89558113 17.\n 36.         21.         28.         23.         24.         22.\n 31.         46.         23.         28.         39.         26.\n 21.         28.         20.         34.         51.          3.\n 21.          7.30954704 27.94720616 20.68131488 33.         35.05181757\n 44.         30.70572678 34.         18.         30.         10.\n 27.94720616 21.         29.         28.         18.         29.78279613\n 28.         19.         35.2958243  32.         28.         26.68916849\n 42.         17.         50.         14.         21.         24.\n 64.         31.         45.         20.         25.         28.\n 23.47643239  4.         13.         34.          5.         52.\n 36.         24.10899881 30.         49.         30.70572678 29.\n 65.         44.06001827 50.         35.2958243  48.         34.\n 47.         48.         30.70572678 38.         35.05181757 56.\n 19.89558113  0.75       29.78279613 38.         33.         23.\n 22.         44.65953056 34.         29.         22.          2.\n  9.         35.05181757 50.         63.         25.          7.30954704\n 35.         58.         30.          9.         24.10899881 21.\n 55.         71.         21.         18.19479484 54.         25.7554753\n 25.         24.         17.         21.         26.82073281 37.\n 16.         18.         33.         49.8994093  28.         26.\n 29.         30.70572678 36.         54.         24.         47.\n 34.         34.17374861 36.         32.         30.         22.\n 29.51820514 44.         22.87630686 40.5        50.         38.5155\n 39.         23.          2.         22.87630686 17.          8.01370298\n 30.          7.         45.         30.         24.33021696 22.\n 36.          9.         11.         32.         50.         64.\n 19.         32.79502428 33.          8.         17.         27.\n 26.3814249  22.         22.         62.         48.         38.5155\n 39.         36.         35.2958243  40.         28.         30.70572678\n 30.70572678 24.         19.         29.         22.87630686 32.\n 62.         53.         36.         35.2958243  16.         19.\n 34.         39.         20.07016773 32.         25.         39.\n 54.         36.         29.04293865 18.         47.         60.\n 22.         30.70572678 35.         52.         47.         27.45639428\n 37.         36.         24.40235514 49.         29.51820514 49.\n 24.         27.94720616 37.51615833 44.         35.         36.\n 30.         27.         22.         40.         39.         27.86873699\n 33.12898535 35.2958243  35.         24.         34.         26.\n  4.         26.         27.         42.         20.         21.\n 21.         61.         57.         21.         26.         19.89558113\n 80.         51.         32.         38.83477484  9.         28.\n 32.         31.         41.         26.68916849 20.         24.\n  2.         29.56889809  0.75       48.         19.         56.\n 31.10838452 23.         27.94720616 18.         21.         26.3814249\n 18.         24.         27.94720616 32.         23.         58.\n 50.         40.         47.         36.         20.         32.\n 25.         27.51545426 43.         39.08817814 40.         31.\n 70.         31.         35.05181757 18.         24.5        18.\n 43.         36.         23.47643239 27.         20.         14.\n 60.         25.         14.         19.         18.         15.\n 31.          4.         29.56889809 25.         60.         52.\n 44.         19.89558113 49.         42.         18.         35.\n 18.         25.         26.         39.         45.         42.\n 22.         17.09991595 24.         49.8994093  48.         29.\n 52.         19.         38.         27.         27.30887391 33.\n  6.         17.         34.         50.         27.         20.\n 30.         19.89558113 25.         25.         29.         11.\n 35.05181757 23.         23.         28.5        48.         35.\n 27.94720616 27.94720616 38.42663175 36.         21.         24.\n 31.         70.         16.         30.         19.         31.\n  4.          6.         33.         23.         48.          0.67\n 28.         18.         34.         33.         24.33021696 41.\n 20.         36.         16.         51.         39.17490238 30.5\n 33.55117591 32.         24.         48.         57.         29.51820514\n 54.         18.         35.2958243   5.         19.89558113 43.\n 13.         17.         29.         16.1939502  25.         25.\n 18.          8.          1.         46.         35.2958243  16.\n 10.86986696 50.38328427 25.         39.         49.         31.\n 30.         30.         34.         31.         11.          0.42\n 27.         31.         39.         18.         39.         33.\n 26.         39.         35.          6.         30.5        38.83477484\n 23.         31.         43.         10.         52.         27.\n 38.         27.          2.         35.09787898 29.56889809  1.\n 35.2958243  62.         15.          0.83       22.87630686 23.\n 18.         39.         21.         30.70572678 32.         50.91095013\n 20.         16.         30.         34.5        17.         42.\n 10.86986696 35.         28.         43.96476448  4.         74.\n  9.         16.         44.         18.         45.         51.\n 24.         22.87630686 41.         21.         48.         10.86986696\n 24.         42.         27.         31.         25.97788916  4.\n 26.         47.         33.         47.         28.         15.\n 20.         19.         27.94720616 56.         25.         33.\n 22.         28.         25.         39.         27.         19.\n 16.1939502  26.         32.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# 接下来我们要接着做一些数据预处理的工作，比如scaling，将一些变化幅度较大的特征化到[-1,1]之内\n",
    "# 这样可以加速logistic regression的收敛\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "age_scale_param = scaler.fit(df['Age'])\n",
    "df['Age_scaled'] = scaler.fit_transform(df['Age'], age_scale_param)\n",
    "fare_scale_param = scaler.fit(df['Fare'])\n",
    "df['Fare_scaled'] = scaler.fit_transform(df['Fare'], fare_scale_param)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "    \n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来咱们对训练集和测试集做一样的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'], age_scale_param)\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'], fare_scale_param)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"logistic_regression_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=red>0.76555，恩，结果还不错。毕竟，这只是我们简单分析过后出的一个baseline系统嘛</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要判定一下当前模型所处状态(欠拟合or过拟合)\n",
    "\n",
    "<font color=red>有一个很可能发生的问题是，我们不断地做feature engineering，产生的特征越来越多，用这些特征去训练模型，会对我们的训练集拟合得越来越好，同时也可能在逐步丧失泛化能力，从而在待预测的数据上，表现不佳，也就是发生过拟合问题。<font><br>\n",
    "\n",
    "<font color=red>从另一个角度上说，如果模型在待预测的数据上表现不佳，除掉上面说的过拟合问题，也有可能是欠拟合问题，也就是说在训练集上，其实拟合的也不是那么好。<font><br>\n",
    "\n",
    "<font color=red>额，这个欠拟合和过拟合怎么解释呢。这么说吧：<font><br>\n",
    "\n",
    "1. <font color=red>过拟合就像是你班那个学数学比较刻板的同学，老师讲过的题目，一字不漏全记下来了，于是老师再出一样的题目，分分钟精确出结果。but数学考试，因为总是碰到新题目，所以成绩不咋地。<font>\n",
    "2. <font color=red>欠拟合就像是，咳咳，和博主level差不多的差生。连老师讲的练习题也记不住，于是连老师出一样题目复习的周测都做不好，考试更是可想而知了。<font>\n",
    "\n",
    "<font color=red>而在机器学习的问题上，对于过拟合和欠拟合两种情形。我们优化的方式是不同的。<font><br>\n",
    "\n",
    "<font color=red>对过拟合而言，通常以下策略对结果优化是有用的：<font><br>\n",
    "\n",
    "* <font color=red>做一下feature selection，挑出较好的feature的subset来做training\n",
    "* <font color=red>提供更多的数据，从而弥补原始数据的bias问题，学习到的model也会更准确\n",
    "\n",
    "<font color=red>而对于欠拟合而言，我们通常需要更多的feature，更复杂的模型来提高准确度。<font><br>\n",
    "\n",
    "<font color=red>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)<font><br>\n",
    "\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/high_variance.png?imageView/2/w/400/q/100)\n",
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/high_bias.png?imageView/2/w/400/q/100)\n",
    "\n",
    "<font color=red>著名的learning curve可以帮我们判定我们的模型现在所处的状态。我们以样本数为横坐标，训练和交叉验证集上的错误率作为纵坐标，两种状态分别如下两张图所示：过拟合(overfitting/high variace)，欠拟合(underfitting/high bias)<font><br>\n",
    "\n",
    "<font color=red>我们也可以把错误率替换成准确率(得分)，得到另一种形式的learning curve(sklearn 里面是这么做的)。<font><br>\n",
    "\n",
    "<font color=red>回到我们的问题，我们用scikit-learn里面的learning_curve来帮我们分辨我们模型的状态。举个例子，这里我们一起画一下我们最先得到的baseline model的learning curve。<font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "# 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, \n",
    "                        train_sizes=np.linspace(.05, 1., 20), verbose=0, plot=True):\n",
    "    \"\"\"\n",
    "    画出data在某模型上的learning curve.\n",
    "    参数解释\n",
    "    ----------\n",
    "    estimator : 你用的分类器。\n",
    "    title : 表格的标题。\n",
    "    X : 输入的feature，numpy类型\n",
    "    y : 输入的target vector\n",
    "    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点\n",
    "    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)\n",
    "    n_jobs : 并行的的任务数(默认1)\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(u\"训练样本数\")\n",
    "        plt.ylabel(u\"得分\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=u\"训练集上得分\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=u\"交叉验证集上得分\")\n",
    "    \n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "        plt.draw()\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "    \n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff\n",
    "\n",
    "plot_learning_curve(clf, u\"学习曲线\", X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/2_titanic/learning_curve.png?imageView/2/w/600/q/100)\n",
    "<font color=red>在实际数据上看，我们得到的learning curve没有理论推导的那么光滑哈，但是可以大致看出来，训练集和交叉验证集上的得分曲线走势还是符合预期的。<font><br>\n",
    "\n",
    "<font color=red>目前的曲线看来，我们的model并不处于overfitting的状态(overfitting的表现一般是训练集上得分高，而交叉验证集上要低很多，中间的gap比较大)。因此我们可以再做些feature engineering的工作，添加一些新产出的特征或者组合特征到模型中。<font><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>接下来，我们就该看看如何优化baseline系统了<br>\n",
    "我们还有些特征可以再挖掘挖掘<br><br>\n",
    "\n",
    "1. 比如说Name和Ticket两个属性被我们完整舍弃了(好吧，其实是一开始我们对于这种，每一条记录都是一个完全不同的值的属性，并没有很直接的处理方式)<br>\n",
    "2. 比如说，我们想想，年龄的拟合本身也未必是一件非常靠谱的事情<br>\n",
    "3. 另外，以我们的日常经验，小盆友和老人可能得到的照顾会多一些，这样看的话，年龄作为一个连续值，给一个固定的系数，似乎体现不出两头受照顾的实际情况，所以，说不定我们把年龄离散化，按区段分作类别属性会更合适一些<br>\n",
    "\n",
    "那怎么样才知道，哪些地方可以优化，哪些优化的方法是promising的呢？<br>\n",
    "是的<br><br>\n",
    "\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br>\n",
    "要做交叉验证(cross validation)!<br><br>\n",
    "\n",
    "重要的事情说3编！！！<br>\n",
    "因为test.csv里面并没有Survived这个字段(好吧，这是废话，这明明就是我们要预测的结果)，我们无法在这份数据上评定我们算法在该场景下的效果。。。<br>\n",
    "我们通常情况下，这么做cross validation：把train.csv分成两部分，一部分用于训练我们需要的模型，另外一部分数据上看我们预测算法的效果。<br>\n",
    "我们可以用scikit-learn的cross_validation来完成这个工作</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>在此之前，咱们可以看看现在得到的模型的系数，因为系数和它们最终的判定能力强弱是正相关的</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"columns\":list(train_df.columns)[1:], \"coef\":list(clf.coef_.T)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上面的系数和最后的结果是一个正相关的关系<br>\n",
    "我们先看看那些权重绝对值非常大的feature，在我们的模型上：<br>\n",
    "\n",
    "* Sex属性，如果是female会极大提高最后获救的概率，而male会很大程度拉低这个概率。\n",
    "* Pclass属性，1等舱乘客最后获救的概率会上升，而乘客等级为3会极大地拉低这个概率。\n",
    "* 有Cabin值会很大程度拉升最后获救概率(这里似乎能看到了一点端倪，事实上从最上面的有无Cabin记录的Survived分布图上看出，即使有Cabin记录的乘客也有一部分遇难了，估计这个属性上我们挖掘还不够)\n",
    "* Age是一个负相关，意味着在我们的模型里，年龄越小，越有获救的优先权(还得回原数据看看这个是否合理）\n",
    "* 有一个登船港口S会很大程度拉低获救的概率，另外俩港口压根就没啥作用(这个实际上非常奇怪，因为我们从之前的统计图上并没有看到S港口的获救率非常低，所以也许可以考虑把登船港口这个feature去掉试试)。\n",
    "* 船票Fare有小幅度的正相关(并不意味着这个feature作用不大，有可能是我们细化的程度还不够，举个例子，说不定我们得对它离散化，再分至各个乘客等级上？)\n",
    "\n",
    "噢啦，观察完了，我们现在有一些想法了，但是怎么样才知道，哪些优化的方法是promising的呢？<br>\n",
    "\n",
    "恩，要靠交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# 简单看看打分情况\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "all_data = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "X = all_data.as_matrix()[:,1:]\n",
    "y = all_data.as_matrix()[:,0]\n",
    "print cross_validation.cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "\n",
    "# 分割数据\n",
    "split_train, split_cv = cross_validation.train_test_split(\u001d",
    "df, test_size=0.3, random_state=0)\n",
    "train_df = split_train.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "# 生成模型\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(train_df.as_matrix()[:,1:], train_df.as_matrix()[:,0])\n",
    "\n",
    "\n",
    "\n",
    "# 对cross validation数据进行预测\n",
    "\n",
    "cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')\n",
    "predictions = clf.predict(cv_df.as_matrix()[:,1:])\n",
    "split_cv[ predictions != cv_df.as_matrix()[:,0] ].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除预测错误的case看原始dataframe数据\n",
    "#split_cv['PredictResult'] = predictions\n",
    "origin_data_train = pd.read_csv(\"Train.csv\")\n",
    "bad_cases = origin_data_train.loc[origin_data_train['PassengerId'].isin(split_cv[predictions != cv_df.as_matrix()[:,0]]['PassengerId'].values)]\n",
    "bad_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比bad case，我们仔细看看我们预测错的样本，到底是哪些特征有问题，咱们处理得还不够细？<br>\n",
    "\n",
    "我们随便列一些可能可以做的优化操作：<br>\n",
    "\n",
    "* Age属性不使用现在的拟合方式，而是根据名称中的『Mr』『Mrs』『Miss』等的平均值进行填充。\n",
    "* Age不做成一个连续值属性，而是使用一个步长进行离散化，变成离散的类目feature。\n",
    "* Cabin再细化一些，对于有记录的Cabin属性，我们将其分为前面的字母部分(我猜是位置和船层之类的信息) 和 后面的数字部分(应该是房间号，有意思的事情是，如果你仔细看看原始数据，你会发现，这个值大的情况下，似乎获救的可能性高一些)。\n",
    "* Pclass和Sex俩太重要了，我们试着用它们去组出一个组合属性来试试，这也是另外一种程度的细化。\n",
    "* 单加一个Child字段，Age<=12的，设为1，其余为0(你去看看数据，确实小盆友优先程度很高啊)\n",
    "* 如果名字里面有『Mrs』，而Parch>1的，我们猜测她可能是一个母亲，应该获救的概率也会提高，因此可以多加一个Mother字段，此种情况下设为1，其余情况下设为0\n",
    "* 登船港口可以考虑先去掉试试(Q和C本来就没权重，S有点诡异)\n",
    "* 把堂兄弟/兄妹 和 Parch 还有自己 个数加在一起组一个Family_size字段(考虑到大家族可能对最后的结果有影响)\n",
    "* Name是一个我们一直没有触碰的属性，我们可以做一些简单的处理，比如说男性中带某些字眼的(‘Capt’, ‘Don’, ‘Major’, ‘Sir’)可以统一到一个Title，女性也一样。\n",
    "\n",
    "大家接着往下挖掘，可能还可以想到更多可以细挖的部分。我这里先列这些了，然后我们可以使用手头上的”train_df”和”cv_df”开始试验这些feature engineering的tricks是否有效了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[data_train['Name'].str.contains(\"Major\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Train.csv\")\n",
    "data_train['Sex_Pclass'] = data_train.Sex + \"_\" + data_train.Pclass.map(str)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "### 使用 RandomForestClassifier 填补缺失的年龄属性\n",
    "def set_missing_ages(df):\n",
    "    \n",
    "    # 把已有的数值型特征取出来丢进Random Forest Regressor中\n",
    "    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "\n",
    "    # 乘客分成已知年龄和未知年龄两部分\n",
    "    known_age = age_df[age_df.Age.notnull()].as_matrix()\n",
    "    unknown_age = age_df[age_df.Age.isnull()].as_matrix()\n",
    "\n",
    "    # y即目标年龄\n",
    "    y = known_age[:, 0]\n",
    "\n",
    "    # X即特征属性值\n",
    "    X = known_age[:, 1:]\n",
    "\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n",
    "    rfr.fit(X, y)\n",
    "    \n",
    "    # 用得到的模型进行未知年龄结果预测\n",
    "    predictedAges = rfr.predict(unknown_age[:, 1::])\n",
    "    \n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n",
    "    \n",
    "    return df, rfr\n",
    "\n",
    "def set_Cabin_type(df):\n",
    "    df.loc[ (df.Cabin.notnull()), 'Cabin' ] = \"Yes\"\n",
    "    df.loc[ (df.Cabin.isnull()), 'Cabin' ] = \"No\"\n",
    "    return df\n",
    "\n",
    "data_train, rfr = set_missing_ages(data_train)\n",
    "data_train = set_Cabin_type(data_train)\n",
    "\n",
    "dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_train['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "age_scale_param = scaler.fit(df['Age'])\n",
    "df['Age_scaled'] = scaler.fit_transform(df['Age'], age_scale_param)\n",
    "fare_scale_param = scaler.fit(df['Fare'])\n",
    "df['Fare_scaled'] = scaler.fit_transform(df['Fare'], fare_scale_param)\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到RandomForestRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "clf.fit(X, y)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0\n",
    "data_test['Sex_Pclass'] = data_test.Sex + \"_\" + data_test.Pclass.map(str)\n",
    "# 接着我们对test_data做和train_data中一致的特征变换\n",
    "# 首先用同样的RandomForestRegressor模型填上丢失的年龄\n",
    "tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]\n",
    "null_age = tmp_df[data_test.Age.isnull()].as_matrix()\n",
    "# 根据特征属性X预测年龄并补上\n",
    "X = null_age[:, 1:]\n",
    "predictedAges = rfr.predict(X)\n",
    "data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges\n",
    "\n",
    "data_test = set_Cabin_type(data_test)\n",
    "dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')\n",
    "dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')\n",
    "dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')\n",
    "dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')\n",
    "dummies_Sex_Pclass = pd.get_dummies(data_test['Sex_Pclass'], prefix= 'Sex_Pclass')\n",
    "\n",
    "\n",
    "df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass, dummies_Sex_Pclass], axis=1)\n",
    "df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Sex_Pclass'], axis=1, inplace=True)\n",
    "df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'], age_scale_param)\n",
    "df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'], fare_scale_param)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*')\n",
    "predictions = clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>一般做到后期，咱们要进行模型优化的方法就是模型融合啦<br>\n",
    "先解释解释啥叫模型融合哈，我们还是举几个例子直观理解一下好了。<br><br>\n",
    "\n",
    "大家都看过知识问答的综艺节目中，求助现场观众时候，让观众投票，最高的答案作为自己的答案的形式吧，每个人都有一个判定结果，最后我们相信答案在大多数人手里。<br>\n",
    "\n",
    "再通俗一点举个例子。你和你班某数学大神关系好，每次作业都『模仿』他的，于是绝大多数情况下，他做对了，你也对了。突然某一天大神脑子犯糊涂，手一抖，写错了一个数，于是…恩，你也只能跟着错了。 <br>\n",
    "我们再来看看另外一个场景，你和你班5个数学大神关系都很好，每次都把他们作业拿过来，对比一下，再『自己做』，那你想想，如果哪天某大神犯糊涂了，写错了，but另外四个写对了啊，那你肯定相信另外4人的是正确答案吧？<br>\n",
    "\n",
    "最简单的模型融合大概就是这么个意思，比如分类问题，当我们手头上有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)，那我们让他们都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果。<br>\n",
    "\n",
    "bingo，问题就这么完美的解决了。<br>\n",
    "\n",
    "模型融合可以比较好地缓解，训练过程中产生的过拟合问题，从而对于结果的准确度提升有一定的帮助。<br>\n",
    "\n",
    "话说回来，回到我们现在的问题。你看，我们现在只讲了logistic regression，如果我们还想用这个融合思想去提高我们的结果，我们该怎么做呢？<br>\n",
    "\n",
    "既然这个时候模型没得选，那咱们就在数据上动动手脚咯。大家想想，如果模型出现过拟合现在，一定是在我们的训练上出现拟合过度造成的对吧。<br>\n",
    "\n",
    "那我们干脆就不要用全部的训练集，每次取训练集的一个subset，做训练，这样，我们虽然用的是同一个机器学习算法，但是得到的模型却是不一样的；同时，因为我们没有任何一份子数据集是全的，因此即使出现过拟合，也是在子训练集上出现过拟合，而不是全体数据上，这样做一个融合，可能对最后的结果有一定的帮助。对，这就是常用的Bagging。<br>\n",
    "\n",
    "我们用scikit-learn里面的Bagging来完成上面的思路，过程非常简单。代码如下：<br><br><font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "train_np = train_df.as_matrix()\n",
    "\n",
    "# y即Survival结果\n",
    "y = train_np[:, 0]\n",
    "\n",
    "# X即特征属性值\n",
    "X = train_np[:, 1:]\n",
    "\n",
    "# fit到BaggingRegressor之中\n",
    "clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\n",
    "bagging_clf = BaggingRegressor(clf, n_estimators=10, max_samples=0.8, max_features=1.0, bootstrap=True, bootstrap_features=False, n_jobs=-1)\n",
    "bagging_clf.fit(X, y)\n",
    "\n",
    "test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass.*|Mother|Child|Family|Title')\n",
    "predictions = bagging_clf.predict(test)\n",
    "result = pd.DataFrame({'PassengerId':data_test['PassengerId'].as_matrix(), 'Survived':predictions.astype(np.int32)})\n",
    "result.to_csv(\"/Users/MLS/Downloads/logistic_regression_predictions2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>下面是咱们用别的分类器解决这个问题的代码：</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import  DataFrame\n",
    "from patsy import dmatrices\n",
    "import string\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split,StratifiedShuffleSplit,StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "##Read configuration parameters\n",
    "\n",
    "train_file=\"train.csv\"\n",
    "MODEL_PATH=\"./\"\n",
    "test_file=\"test.csv\"\n",
    "SUBMISSION_PATH=\"./\"\n",
    "seed= 0\n",
    "\n",
    "print train_file,seed\n",
    "\n",
    "# 输出得分\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "#清理和处理数据\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if string.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    print big_string\n",
    "    return np.nan\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "enc=preprocessing.OneHotEncoder()\n",
    "\n",
    "def clean_and_munge_data(df):\n",
    "    #处理缺省值\n",
    "    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n",
    "    #处理一下名字，生成Title字段\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                'Don', 'Jonkheer']\n",
    "    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "\n",
    "    #处理特殊的称呼，全处理成mr, mrs, miss, master\n",
    "    def replace_titles(x):\n",
    "        title=x['Title']\n",
    "        if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "            return 'Mr'\n",
    "        elif title in ['Master']:\n",
    "            return 'Master'\n",
    "        elif title in ['Countess', 'Mme','Mrs']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms','Miss']:\n",
    "            return 'Miss'\n",
    "        elif title =='Dr':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        elif title =='':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Master'\n",
    "            else:\n",
    "                return 'Miss'\n",
    "        else:\n",
    "            return title\n",
    "\n",
    "    df['Title']=df.apply(replace_titles, axis=1)\n",
    "\n",
    "    #看看家族是否够大，咳咳\n",
    "    df['Family_Size']=df['SibSp']+df['Parch']\n",
    "    df['Family']=df['SibSp']*df['Parch']\n",
    "\n",
    "\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==1),'Fare'] =np.median(df[df['Pclass'] == 1]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==2),'Fare'] =np.median( df[df['Pclass'] == 2]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==3),'Fare'] = np.median(df[df['Pclass'] == 3]['Fare'].dropna())\n",
    "\n",
    "    df['Gender'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "    df['AgeFill']=df['Age']\n",
    "    mean_ages = np.zeros(4)\n",
    "    mean_ages[0]=np.average(df[df['Title'] == 'Miss']['Age'].dropna())\n",
    "    mean_ages[1]=np.average(df[df['Title'] == 'Mrs']['Age'].dropna())\n",
    "    mean_ages[2]=np.average(df[df['Title'] == 'Mr']['Age'].dropna())\n",
    "    mean_ages[3]=np.average(df[df['Title'] == 'Master']['Age'].dropna())\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Miss') ,'AgeFill'] = mean_ages[0]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mrs') ,'AgeFill'] = mean_ages[1]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mr') ,'AgeFill'] = mean_ages[2]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Master') ,'AgeFill'] = mean_ages[3]\n",
    "\n",
    "    df['AgeCat']=df['AgeFill']\n",
    "    df.loc[ (df.AgeFill<=10) ,'AgeCat'] = 'child'\n",
    "    df.loc[ (df.AgeFill>60),'AgeCat'] = 'aged'\n",
    "    df.loc[ (df.AgeFill>10) & (df.AgeFill <=30) ,'AgeCat'] = 'adult'\n",
    "    df.loc[ (df.AgeFill>30) & (df.AgeFill <=60) ,'AgeCat'] = 'senior'\n",
    "\n",
    "    df.Embarked = df.Embarked.fillna('S')\n",
    "\n",
    "\n",
    "    df.loc[ df.Cabin.isnull()==True,'Cabin'] = 0.5\n",
    "    df.loc[ df.Cabin.isnull()==False,'Cabin'] = 1.5\n",
    "\n",
    "    df['Fare_Per_Person']=df['Fare']/(df['Family_Size']+1)\n",
    "\n",
    "    #Age times class\n",
    "\n",
    "    df['AgeClass']=df['AgeFill']*df['Pclass']\n",
    "    df['ClassFare']=df['Pclass']*df['Fare_Per_Person']\n",
    "\n",
    "\n",
    "    df['HighLow']=df['Pclass']\n",
    "    df.loc[ (df.Fare_Per_Person<8) ,'HighLow'] = 'Low'\n",
    "    df.loc[ (df.Fare_Per_Person>=8) ,'HighLow'] = 'High'\n",
    "\n",
    "\n",
    "\n",
    "    le.fit(df['Sex'] )\n",
    "    x_sex=le.transform(df['Sex'])\n",
    "    df['Sex']=x_sex.astype(np.float)\n",
    "\n",
    "    le.fit( df['Ticket'])\n",
    "    x_Ticket=le.transform( df['Ticket'])\n",
    "    df['Ticket']=x_Ticket.astype(np.float)\n",
    "\n",
    "    le.fit(df['Title'])\n",
    "    x_title=le.transform(df['Title'])\n",
    "    df['Title'] =x_title.astype(np.float)\n",
    "\n",
    "    le.fit(df['HighLow'])\n",
    "    x_hl=le.transform(df['HighLow'])\n",
    "    df['HighLow']=x_hl.astype(np.float)\n",
    "\n",
    "\n",
    "    le.fit(df['AgeCat'])\n",
    "    x_age=le.transform(df['AgeCat'])\n",
    "    df['AgeCat'] =x_age.astype(np.float)\n",
    "\n",
    "    le.fit(df['Embarked'])\n",
    "    x_emb=le.transform(df['Embarked'])\n",
    "    df['Embarked']=x_emb.astype(np.float)\n",
    "\n",
    "    df = df.drop(['PassengerId','Name','Age','Cabin'], axis=1) #remove Name,Age and PassengerId\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "#读取数据\n",
    "traindf=pd.read_csv(train_file)\n",
    "##清洗数据\n",
    "df=clean_and_munge_data(traindf)\n",
    "########################################formula################################\n",
    " \n",
    "formula_ml='Survived~Pclass+C(Title)+Sex+C(AgeCat)+Fare_Per_Person+Fare+Family_Size' \n",
    "\n",
    "y_train, x_train = dmatrices(formula_ml, data=df, return_type='dataframe')\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "print y_train.shape,x_train.shape\n",
    "\n",
    "##选择训练和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2,random_state=seed)\n",
    "#初始化分类器\n",
    "clf=RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=5, min_samples_split=1,\n",
    "  min_samples_leaf=1, max_features='auto',    bootstrap=False, oob_score=False, n_jobs=1, random_state=seed,\n",
    "  verbose=0)\n",
    "\n",
    "###grid search找到最好的参数\n",
    "param_grid = dict( )\n",
    "##创建分类pipeline\n",
    "pipeline=Pipeline([ ('clf',clf) ])\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=3,scoring='accuracy',\\\n",
    "cv=StratifiedShuffleSplit(Y_train, n_iter=10, test_size=0.2, train_size=None, indices=None, \\\n",
    "random_state=seed, n_iterations=None)).fit(X_train, Y_train)\n",
    "# 对结果打分\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)\n",
    "report(grid_search.grid_scores_)\n",
    " \n",
    "print('-----grid search end------------')\n",
    "print ('on all train set')\n",
    "scores = cross_val_score(grid_search.best_estimator_, x_train, y_train,cv=3,scoring='accuracy')\n",
    "print scores.mean(),scores\n",
    "print ('on test set')\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_test, Y_test,cv=3,scoring='accuracy')\n",
    "print scores.mean(),scores\n",
    "\n",
    "# 对结果打分\n",
    "\n",
    "print(classification_report(Y_train, grid_search.best_estimator_.predict(X_train) ))\n",
    "print('test data')\n",
    "print(classification_report(Y_test, grid_search.best_estimator_.predict(X_test) ))\n",
    "\n",
    "model_file=MODEL_PATH+'model-rf.pkl'\n",
    "joblib.dump(grid_search.best_estimator_, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
